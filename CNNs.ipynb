{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.functional as F\n",
    "\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# import os\n",
    "#\n",
    "# import torch\n",
    "# import torchvision\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.functional as F\n",
    "#\n",
    "# from torchsummary import summary\n",
    "# from torchvision import transforms\n",
    "#\n",
    "# # Loading the DataSet\n",
    "# labels = ['Negative', 'Positive']\n",
    "# # size = 216 # Image is uniformly resized to (216 x 216)\n",
    "# size = 120 # Image is uniformly resized to (216 x 216)\n",
    "# def GenerateData(PATH):\n",
    "#     data = []\n",
    "#     for label in labels:\n",
    "#         path = os.path.join(PATH, label)\n",
    "#         category = labels.index(label)\n",
    "#         for img in os.listdir(path):\n",
    "#             try:\n",
    "#                 img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "#                 img_arr = cv2.resize(img_arr, (size, size))\n",
    "#                 data.append([img_arr, category])\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#     return np.array(data)\n",
    "#\n",
    "# Dataset = GenerateData('DataSet')\n",
    "#\n",
    "# img = []\n",
    "# labels = []\n",
    "# for i, j in Dataset :\n",
    "#     img.append(i)\n",
    "#     labels.append(j)\n",
    "#\n",
    "# # img = np.array(img).reshape(-1, 1, 216, 216) # One Channel\n",
    "# img = np.array(img).reshape(-1, 1, 120, 120) # One Channel\n",
    "# img = img/255 #Normalization\n",
    "# labels = np.array(labels)\n",
    "#\n",
    "# # Converting the Images and Labels to Tensor\n",
    "# img = torch.from_numpy(img)\n",
    "# img = img.to(torch.float)\n",
    "# labels = torch.from_numpy(labels)\n",
    "# labels = labels.to(torch.float)\n",
    "#\n",
    "# # Defining the CNN\n",
    "# class ConvolutionalNeuralNetwork(nn.Module) :\n",
    "#     def __init__(self):\n",
    "#         super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "#         self.conv1 = nn.Sequential(\n",
    "#             nn.Conv2d(1, 64, 3, 1, 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(2) #(64, 60, 60) / (64, 108, 108)\n",
    "#         )\n",
    "#         self.conv2 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 64, 3, 1, 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(2) #(64, 30, 30) / (64, 54, 54)\n",
    "#         )\n",
    "#         self.conv3 = nn.Sequential(\n",
    "#             nn.Conv2d(64, 128, 3, 1, 1),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.MaxPool2d(2) #(128, 15, 15) / (128, 27, 27)\n",
    "#         )\n",
    "#\n",
    "#         # self.fc1 = nn.Sequential(\n",
    "#         #     # nn.Linear(128*27*27, 1024),\n",
    "#         #     nn.Linear(128*15*15, 1024),\n",
    "#         #     nn.ReLU(inplace = True)\n",
    "#         # )\n",
    "#         self.fc2 = nn.Sequential(\n",
    "#             nn.Linear(128*15*15, 256),\n",
    "#             nn.ReLU(inplace = True),\n",
    "#             nn.Dropout(0.5),\n",
    "#             nn.BatchNorm1d(256)\n",
    "#         )\n",
    "#         self.fc3 = nn.Sequential(\n",
    "#             nn.Linear(256, 2),\n",
    "#             nn.Softmax(dim = 1)\n",
    "#         )\n",
    "#\n",
    "#     def forward(self, x):\n",
    "#         x = self.conv1(x)\n",
    "#         x = self.conv2(x)\n",
    "#         x = self.conv3(x)\n",
    "#\n",
    "#         # x = x.view(-1, 128*27*27)\n",
    "#         x = x.view(-1, 128*15*15)\n",
    "#         # x = self.fc1(x)\n",
    "#         x = self.fc2(x)\n",
    "#         x = self.fc3(x)\n",
    "#\n",
    "#         return x\n",
    "#\n",
    "# # Visualizing the CNN\n",
    "# Net = ConvolutionalNeuralNetwork()\n",
    "# # summary(Net, (1, 216, 216))\n",
    "# summary(Net, (1, 120, 120))\n",
    "#\n",
    "# optimizer = optim.Adam(Net.parameters(), lr = 1e-5, weight_decay = 0.005)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "#\n",
    "# # Defining Training Loop\n",
    "# def Train(input, target, EPOCH) :\n",
    "#     for index in range(EPOCH) :\n",
    "#         optimizer.zero_grad()\n",
    "#         output = Net(input)\n",
    "#         loss = criterion(output, target)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         print(f\"Epoch : {index + 1} | Loss : {loss.item()}\")\n",
    "#     torch.save(Net, 'Model.pt')\n",
    "#     print('Model Saved Successfully !')\n",
    "#\n",
    "# Train(img, labels, 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wc/rkrv7ck92zd4f_3qgk8q2gn00000gn/T/ipykernel_17645/1293110571.py:17: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return np.array(data)\n"
     ]
    }
   ],
   "source": [
    "# Loading the DataSet\n",
    "labels = ['Negative', 'Positive']\n",
    "# size = 216 # Image is uniformly resized to (216 x 216)\n",
    "size = 120 # Image is uniformly resized to (216 x 216)\n",
    "def GenerateData(PATH):\n",
    "    data = []\n",
    "    for label in labels:\n",
    "        path = os.path.join(PATH, label)\n",
    "        category = labels.index(label)\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_arr = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                img_arr = cv2.resize(img_arr, (size, size))\n",
    "                data.append([img_arr, category])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "    return np.array(data)\n",
    "\n",
    "Dataset = GenerateData('DataSet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "img = []\n",
    "labels = []\n",
    "for i, j in Dataset :\n",
    "    img.append(i)\n",
    "    labels.append(j)\n",
    "\n",
    "# img = np.array(img).reshape(-1, 1, 216, 216) # One Channel\n",
    "img = np.array(img).reshape(-1, 1, 120, 120) # One Channel\n",
    "img = img/255 #Normalization\n",
    "labels = np.array(labels)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Converting the Images and Labels to Tensor\n",
    "img = torch.from_numpy(img)\n",
    "img = img.to(torch.float)\n",
    "labels = torch.from_numpy(labels)\n",
    "labels = labels.to(torch.float)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([40000, 1, 120, 120])"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Defining the CNN\n",
    "class ConvolutionalNeuralNetwork(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(ConvolutionalNeuralNetwork, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2), #(64, 60, 60) / (64, 108, 108)\n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 64, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2) #(64, 30, 30) / (64, 54, 54)\n",
    "        )\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.MaxPool2d(2) #(128, 15, 15) / (128, 27, 27)\n",
    "        )\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            # nn.Linear(128*27*27, 1024),\n",
    "            nn.Linear(64*15*15, 1024),\n",
    "            nn.ReLU(inplace = True)\n",
    "        )\n",
    "        self.fc2 = nn.Sequential(\n",
    "            nn.Linear(1024, 256),\n",
    "            nn.ReLU(inplace = True),\n",
    "            nn.Dropout(0.5)\n",
    "            # nn.BatchNorm1d(256)\n",
    "        )\n",
    "        self.fc3 = nn.Sequential(\n",
    "            nn.Linear(256, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        # self.fc = nn.Sequential(\n",
    "        #     nn.Linear(120*120, 1)\n",
    "        # )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "\n",
    "        # x = x.view(-1, 128*27*27)\n",
    "        x = x.view(-1, 64*15*15)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        # x = self.fc(x)\n",
    "\n",
    "\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 120, 120]             640\n",
      "              ReLU-2         [-1, 64, 120, 120]               0\n",
      "         MaxPool2d-3           [-1, 64, 60, 60]               0\n",
      "            Conv2d-4           [-1, 64, 60, 60]          36,928\n",
      "              ReLU-5           [-1, 64, 60, 60]               0\n",
      "         MaxPool2d-6           [-1, 64, 30, 30]               0\n",
      "            Conv2d-7          [-1, 128, 30, 30]          73,856\n",
      "              ReLU-8          [-1, 128, 30, 30]               0\n",
      "         MaxPool2d-9          [-1, 128, 15, 15]               0\n",
      "           Linear-10                 [-1, 1024]      14,746,624\n",
      "             ReLU-11                 [-1, 1024]               0\n",
      "           Linear-12                  [-1, 256]         262,400\n",
      "             ReLU-13                  [-1, 256]               0\n",
      "          Dropout-14                  [-1, 256]               0\n",
      "           Linear-15                    [-1, 1]             257\n",
      "          Sigmoid-16                    [-1, 1]               0\n",
      "================================================================\n",
      "Total params: 15,120,705\n",
      "Trainable params: 15,120,705\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.05\n",
      "Forward/backward pass size (MB): 21.77\n",
      "Params size (MB): 57.68\n",
      "Estimated Total Size (MB): 79.51\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Visualizing the CNN\n",
    "Net = ConvolutionalNeuralNetwork()\n",
    "# summary(Net, (1, 216, 216))\n",
    "summary(Net, (1, 120, 120))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(Net.parameters(), lr = 1e-3)\n",
    "criterion = nn.BCELoss()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Defining Training Loop\n",
    "def Train(input, target, EPOCH) :\n",
    "    for index in range(EPOCH) :\n",
    "        optimizer.zero_grad()\n",
    "        output = Net(input)\n",
    "        output = torch.squeeze(output)\n",
    "        loss = criterion(output, target.float())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"Epoch : {index + 1} | Loss : {loss.item()}\")\n",
    "    torch.save(Net, 'Model.pt')\n",
    "    print('Model Saved Successfully !')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Train(img, labels, 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
